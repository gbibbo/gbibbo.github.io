<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-108114467-1', 'auto');
        ga('send', 'pageview');
    </script>
    <meta name=viewport content=“width=800”>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <link href='https://fonts.googleapis.com/css?family=Titillium Web' rel='stylesheet'>
    <meta charset="utf-8">
    <style type="text/css">
        /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 400
        }

        heading {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 19px;
            font-weight: 1000
        }

        strong {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 800
        }

        strongred {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            color: 'red';
            font-size: 16px
        }

        sectionheading {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 22px;
            font-weight: 600
        }
    </style>
    <link rel="shortcut icon" type="image/png" href="homepage_files/xubo1.jpeg"/>
    <title>Xubo Liu</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body>
<table width="1000" border="0" align="center" cellspacing="0" cellpadding="20">
    <tr>
        <td halign="center">
            <p align="center">
                <font size="6"> Xubo Liu 刘徐博</font>
            </p>
        </td>
    </tr>
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle">
                        <p>I am a final-year Ph.D. student in  <a href="https://www.surrey.ac.uk/centre-vision-speech-signal-processing">
                            Centre for Vision, Speech & Singal Processing </a> at <a href="https://www.surrey.ac.uk/">University of Surrey</a>
                            advised by <a href="https://www.surrey.ac.uk/people/wenwu-wang">Prof. Wenwu Wang</a> 
                            and <a href="https://www.surrey.ac.uk/people/mark-plumbley">Prof. Mark D. Plumbley</a>.
                            My passion is to build AI models to understand the world with multi-modalities and engage with humans.
                            Currently, I work on computational auditory scene analysis, multimodal content creation and large language models for audo/speech/music signals.
                        </p>
                        <p> Previously,  I spent four months working with <a href="https://www.linkedin.com/in/fuegen/">Dr. Christian Fuegen</a>
                            and <a href="https://www.linkedin.com/in/egor-lakomkin-1032b337/">Dr. Egor Lakomkin</a> at Meta AI, London. 
                            During my PhD, I worked closely with <a href="https://qiuqiangkong.github.io/">Dr. Qiuqiang Kong</a> at the  
                            <a href="https://www.cuhk.edu.hk/english/index.html">Chinese University of Hong Kong (CUHK)</a>.
                            I graduated with First Class Honors from <a href="https://www.qmul.ac.uk/"></a>Queen Mary University of London</a> 
                            in 2020 with a BSc in Telecommunications Engineering.
                        </p>
                        
                        
                            <p> I am open to research collaborations. Please feel free to email me.</p>

                            <p>
                                <b>Email:</b>
                                xubo.liu@surrey.ac.uk
                                <br/>
                            </p>

                        <p>
                            <b>Personal:</b>
                            <a href="https://scholar.google.com/citations?user=-OlNYSgAAAAJ">[Google Scholar]</a> |
                            <a href="https://github.com/liuxubo717">[Github]</a> | 
                            <a href="https://www.linkedin.com/in/xubo-liu-533b62191/">[Linkedin]</a> | 
                            <a href="https://twitter.com/LiuXub">[Twitter]</a>
                        </p>
                    </td>
                    <td width="100%" valign="top">
                        <img src="homepage_files/xubo1.jpeg" width="95%">
                    </td>
                </tr>
            </table>

            
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Publications</heading>
                    </td>
                </tr>
            </table>
            
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                
                <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                    <td width="35%">
                        <div class="one">
                            <div class="two" id='lemara'><img src='homepage_files/results.png' alt="sym" width="100%"
                                                              style="border-style: none"></div>
                        </div>
                        <script type="text/javascript">
                            function lemara_start() {
                                document.getElementById('lemara').style.opacity = "0.9";
                            }

                            function lemara_stop() {
                                document.getElementById('lemara').style.opacity = "1";
                            }

                            friendly_stop()
                        </script>
                    </td>
                    <td valign="top" width="65%">
                        <heading>Separate Anything You Describe</heading>
                        <br><b>Xubo Liu</b>, Qiuqiang Kong, Yan Zhao, Haohe Liu, Yi Yuan, Yuzhuo Liu, Rui Xia, Yuxuan Wang, Mark D Plumbley, Wenwu Wang<br>
                        <em>arXiv:2308.05037<br></em>
                        <a href="https://arxiv.org/pdf/2308.05037.pdf">paper</a> |
                        <a href="https://audio-agi.github.io/Separate-Anything-You-Describe/">project</a> |
                        <a href="https://github.com/Audio-AGI/AudioSep/">code
                        <img src="https://img.shields.io/github/stars/Audio-AGI/AudioSep?style=social" alt="GitHub stars" width="12%"></a>
                        <br>

                        <strong>Media coverage: </strong> <br>
                        <a href="https://twitter.com/Gradio/status/1712854634568761733">
                            <img src="media/gradio.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://www.unite.ai/audiosep-separate-anything-you-describe/">
                            <img src="media/uniteai.jpg"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://replicate.com/cjwbw/audiosep">
                            <img src="media/replicate.jpg"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://www.thedeepview.co/p/filtering-noise">
                            <img src="media/deepview.jpg"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://www.marktechpost.com/2023/08/14/master-key-to-audio-source-separation-introducing-audiosep-to-separate-anything-you-describe/">
                            <img src="media/techpost.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://staffing.archetyp.jp/magazine/audiosep/">
                            <img src="media/staffing.jpg"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a> 
                        <a href="https://cybered.io/insights/the-evolution-of-sound/">
                            <img src="media/cyber.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://aipressroom.com/audiosep-separate-anything-you-describe/?feed_id=6414&_unique_id=652f9fed0139b">
                            <img src="media/pressroom.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://medium.aifastcash.com/introducing-audiosep-a-model-for-sound-extraction-and-separation-using-natural-language-c543b3c8ca1b">
                            <img src="media/medium.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        

                    </td>
                </tr>

                <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                    <td width="35%">
                        <div class="one">
                            <div class="two" id='lemara'><img src='homepage_files/WavJourney.png' alt="sym" width="100%"
                                                              style="border-style: none"></div>
                        </div>
                        <script type="text/javascript">
                            function lemara_start() {
                                document.getElementById('lemara').style.opacity = "0.9";
                            }

                            function lemara_stop() {
                                document.getElementById('lemara').style.opacity = "1";
                            }

                            friendly_stop()
                        </script>
                    </td>
                    <td valign="top" width="65%">
                        <heading>WavJourney: Compositional Audio Creation with Large Language Models</heading>
                        <br><b>Xubo Liu</b>, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D Plumbley, Wenwu Wang<br>
                        <em>arXiv:2307.14335<br></em>
                        <a href="https://arxiv.org/pdf/2307.14335.pdf">paper</a> |
                        <a href="https://audio-agi.github.io/WavJourney_demopage/">project</a> |
                        <a href="https://github.com/Audio-AGI/WavJourney">code
                            <img src="https://img.shields.io/github/stars/Audio-AGI/WavJourney?style=social" alt="GitHub stars" width="12%"></a>
                        </a><br><br>

                        <strong>Media coverage: </strong> <br>
                        <a href="https://twitter.com/Gradio/status/1697258152696365162">
                            <img src="media/gradio.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://www.marktechpost.com/2023/09/09/meet-wavjourney-an-ai-framework-for-compositional-audio-creation-with-large-language-models/">
                            <img src="media/techpost.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://www.kdnuggets.com/wavjourney-a-journey-into-the-world-of-audio-storyline-generation">
                            <img src="media/kd.jpg"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://www.e2enetworks.com/blog/wavjourney-compositional-audio-creation-with-large-language-models">
                            <img src="media/cloud.jpg"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://medium.com/@multiplatform.ai/wavjourney-transforming-multimedia-creation-with-llms-787a698ec97c">
                            <img src="media/medium.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://isp.page/news/the-power-of-multi-modal-ai-in-creating-dynamic-multimedia-content/">
                            <img src="media/isp.jpg"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://www.cjco.com.au/article/news/revolutionizing-multimedia-content-creation-the-potential-of-large-language-models-and-multi-modal-ai-in-audio-narrative-design/">
                            <img src="media/cjco.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href="https://www.techno-edge.net/article/2023/07/31/1671.html">
                            <img src="media/technoedge.png"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                        <a href=" https://hub.baai.ac.cn/view/28146">
                            <img src="media/zhiyuan.jpg"  alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>



                <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                    <td width="35%">
                        <div class="one">
                            <div class="two" id='lemara'><img src='homepage_files/synthvsr.png' alt="sym" width="100%"
                                                                style="border-style: none"></div>
                        </div>
                        <script type="text/javascript">
                            function lemara_start() {
                                document.getElementById('lemara').style.opacity = "0.9";
                            }

                            function lemara_stop() {
                                document.getElementById('lemara').style.opacity = "1";
                            }

                            friendly_stop()
                        </script>
                    </td>
                    <td valign="top" width="65%">
                        <heading>SynthVSR: Scaling Up Visual Speech Recognition with Synthetic Supervision</heading>
                        <br><b>Xubo Liu</b>, Egor Lakomkin, Konstantinos Vougioukas, Pingchuan Ma, Honglie Chen, Ruiming Xie, Morrie Doulaty, Niko Moritz, Jachym Kolar, Stavros Petridis, Maja Pantic, Christian Fuegen<br>
                        <em>CVPR 2023<br></em>
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SynthVSR_Scaling_Up_Visual_Speech_Recognition_With_Synthetic_Supervision_CVPR_2023_paper.pdf">paper</a> |
                        <a href="https://liuxubo717.github.io/SynthVSR/">project</a> 

                    </td>
                </tr>


                <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                    <td width="35%">
                        <div class="one">
                            <div class="two" id='lemara'><img src='homepage_files/VACT.jpg' alt="sym" width="100%"
                                                                style="border-style: none"></div>
                        </div>
                        <script type="text/javascript">
                            function lemara_start() {
                                document.getElementById('lemara').style.opacity = "0.9";
                            }

                            function lemara_stop() {
                                document.getElementById('lemara').style.opacity = "1";
                            }

                            friendly_stop()
                        </script>
                    </td>
                    <td valign="top" width="65%">
                        <heading>Visually-Aware Audio Captioning with Adaptive Audio-Visual Attention</heading>
                        <br><b>Xubo Liu</b>, Qiushi Huang, Xinhao Mei, Haohe Liu, Qiuqiang Kong, Jianyuan Sun, Shengchen Li, Tom Ko, Yu Zhang, Lilian H Tang, Mark D Plumbley, Volkan Kılıç, Wenwu Wang<br>
                        <em>Interspeech 2023<br></em>
                        <a href="https://arxiv.org/pdf/2210.16428.pdf">paper</a> |
                        <a href=" https://github.com/liuxubo717/V-ACT">code
                            <img src="https://img.shields.io/github/stars/liuxubo717/V-ACT?style=social" alt="GitHub stars" width="12%"> 
                        </a>
                       
                    </td>
                </tr>

                <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                    <td width="35%">
                        <div class="one">
                            <div class="two" id='lemara'><img src='homepage_files/simpf.jpg' alt="sym" width="100%"
                                                                style="border-style: none"></div>
                        </div>
                        <script type="text/javascript">
                            function lemara_start() {
                                document.getElementById('lemara').style.opacity = "0.9";
                            }

                            function lemara_stop() {
                                document.getElementById('lemara').style.opacity = "1";
                            }

                            friendly_stop()
                        </script>
                    </td>
                    <td valign="top" width="65%">
                        <heading>Simple Pooling Front-ends For Efficient Audio Classification</heading>
                        <br><b>Xubo Liu</b>, Haohe Liu, Qiuqiang Kong, Xinhao Mei, Mark D. Plumbley, Wenwu Wang<br>
                        <em>ICASSP 2023<br></em>
                        <a href="https://arxiv.org/pdf/2210.00943.pdf">paper</a> |
                        <a href="https://github.com/liuxubo717/SimPFs">code
                            <img src="https://img.shields.io/github/stars/liuxubo717/SimPFs?style=social" alt="GitHub stars" width="12%"> 
                        </a> 

                    </td>
                </tr>

                <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                    <td width="35%">
                        <div class="one">
                            <div class="two" id='lemara'><img src='homepage_files/lass.jpg' alt="sym" width="100%"
                                                                style="border-style: none"></div>
                        </div>
                        <script type="text/javascript">
                            function lemara_start() {
                                document.getElementById('lemara').style.opacity = "0.9";
                            }

                            function lemara_stop() {
                                document.getElementById('lemara').style.opacity = "1";
                            }

                            friendly_stop()
                        </script>
                    </td>
                    <td valign="top" width="65%">
                        <heading>Separate What You Describe: Language-Queried Audio Source Separation</heading>
                        <br><b>Xubo Liu</b>, Haohe Liu, Qiuqiang Kong, Xinhao Mei, Jinzheng Zhao, Qiushi Huang, Mark D Plumbley, Wenwu Wang<br>
                        <em>Interspeech 2022<br></em>
                        <a href="https://arxiv.org/pdf/2203.15147.pdf">paper</a> |
                        <a href="https://liuxubo717.github.io/LASS-demopage/">project</a> |
                        <a href="https://github.com/liuxubo717/LASS">code
                            <img src="https://img.shields.io/github/stars/liuxubo717/LASS?style=social" alt="GitHub stars" width="12%"> 
                        </a> 

                    </td>
                </tr>

                <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                    <td width="35%">
                        <div class="one">
                            <div class="two" id='lemara'><img src='homepage_files/mlsp.jpg' alt="sym" width="100%"
                                                                style="border-style: none"></div>
                        </div>
                        <script type="text/javascript">
                            function lemara_start() {
                                document.getElementById('lemara').style.opacity = "0.9";
                            }

                            function lemara_stop() {
                                document.getElementById('lemara').style.opacity = "1";
                            }

                            friendly_stop()
                        </script>
                    </td>
                    <td valign="top" width="65%">
                        <heading>Conditional Sound Generation Using Neural Discrete Time-Frequency Representation Learning</heading>
                        <br><b>Xubo Liu</b>, Turab Iqbal, Jinzheng Zhao, Qiushi Huang, Mark D Plumbley, Wenwu Wang<br>
                        <em>MLSP 2021<br></em>
                        <a href="https://arxiv.org/pdf/2107.09998.pdf">paper</a> |
                        <a href="https://github.com/liuxubo717/sound_generation">code
                            <img src="https://img.shields.io/github/stars/liuxubo717/sound_generation?style=social" alt="GitHub stars" width="12%"> 
                        </a> 

                    </td>
                </tr>

                <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                    <td width="35%">
                        <div class="one">
                            <div class="two" id='lemara'><img src='homepage_files/cl4ac.png' alt="sym" width="100%"
                                                                style="border-style: none"></div>
                        </div>
                        <script type="text/javascript">
                            function lemara_start() {
                                document.getElementById('lemara').style.opacity = "0.9";
                            }

                            function lemara_stop() {
                                document.getElementById('lemara').style.opacity = "1";
                            }

                            friendly_stop()
                        </script>
                    </td>
                    <td valign="top" width="65%">
                        <heading>CL4AC: A Contrastive Loss for Audio Captioning</heading>
                        <br><b>Xubo Liu</b>, Qiushi Huang, Xinhao Mei, Tom Ko, H Lilian Tang, Mark D Plumbley, Wenwu Wang<br>
                        <em>DCASE Workshop 2021<br></em>
                        <a href="https://arxiv.org/pdf/2107.09990.pdf">paper</a> |
                        <a href="https://github.com/liuxubo717/cl4ac/">code 
                            <img src="https://img.shields.io/github/stars/liuxubo717/cl4ac?style=social" alt="GitHub stars" width="12%">
                        </a> 


                    </td>
                </tr>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <heading>Professional Services</heading><br><br>
                            Special Session Chair of <a href="https://cmsworkshops.com/EUSIPCO2023/view_session.php?SessionID=1060
                            ">Multimodal Learning for Audio and Language"</a> at EUSIPCO 2023<br>
                            Journal reviewer:  IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), International Journal of Computer Vision (IJCV)<br>
                            Conference reviewer: ICML (24), CVPR (24), EMNLP (23), ICASSP (23-24), INTERSPEECH (22-24), MLSP (23)<br>

                        </td>
                    </tr>
                </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td><br>
                        <p align="right"><font size="2">
                            Template credits: <a href="https://unnat.github.io/">Unnat</a>
                        </font></p></td>
                </tr>
                </tbody>
            </table>

        </td>
    </tr>
</table>
</body>
</html>

